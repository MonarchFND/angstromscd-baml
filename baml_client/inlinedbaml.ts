/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Local Ollama client for medical analysis\nclient<llm> LocalOllama {\n  provider openai\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model \"llama3.2:latest\"\n    api_key \"ollama\"\n  }\n}\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.89.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
  "templates/literature-search.baml": "// Literature Search Template\ntemplate_string LiteratureSearchPrompt(research_query: string, domain: string, time_period: string) #\"\nYou are an expert research assistant specializing in scientific literature analysis and medical research.\n\n**Research Query:**\n{{ research_query }}\n\n**Domain/Field:**\n{{ domain }}\n\n**Time Period:**\n{{ time_period }}\n\n**Instructions:**\n1. Analyze the research query and identify key concepts and terms\n2. Suggest relevant search strategies and keywords\n3. Identify potential databases and sources to search\n4. Provide insights on current research trends in this area\n5. Suggest related research questions and areas of investigation\n\n**Output Format:**\nPlease provide a structured literature search strategy with:\n- Key Search Terms and Synonyms\n- Recommended Databases\n- Search Strategy (Boolean operators, filters)\n- Current Research Trends\n- Notable Researchers/Institutions\n- Related Research Areas\n- Potential Gaps in Literature\n\"#\n\nfunction GenerateLiteratureSearch(\n  research_query: string,\n  domain: string,\n  time_period: string\n) -> string {\n  client LocalOllama\n  prompt #\"\n    {{ _.role(\"system\") }}\n    {{ LiteratureSearchPrompt(research_query, domain, time_period) }}\n  \"#\n} ",
  "templates/medical-analysis.baml": "// Medical Analysis Template\ntemplate_string MedicalAnalysisPrompt(patient_data: string, symptoms: string, medical_history: string) #\"\nYou are an expert medical AI assistant specializing in clinical analysis and diagnosis support.\n\n**Patient Data:**\n{{ patient_data }}\n\n**Current Symptoms:**\n{{ symptoms }}\n\n**Medical History:**\n{{ medical_history }}\n\n**Instructions:**\n1. Analyze the provided patient information comprehensively\n2. Identify potential diagnoses based on symptoms and medical history\n3. Suggest relevant diagnostic tests or procedures\n4. Provide risk assessment and recommendations\n5. Include differential diagnoses with probability estimates\n\n**Important:** This analysis is for educational and research purposes only. Always consult with qualified healthcare professionals for actual medical decisions.\n\nPlease provide a structured medical analysis with the following sections:\n- Primary Assessment\n- Differential Diagnoses (with probability estimates)\n- Recommended Diagnostic Tests\n- Risk Factors\n- Treatment Considerations\n- Follow-up Recommendations\n\"#\n\nfunction AnalyzeMedicalCase(\n  patient_data: string,\n  symptoms: string,\n  medical_history: string\n) -> string {\n  client LocalOllama\n  prompt #\"\n    {{ _.role(\"system\") }}\n    {{ MedicalAnalysisPrompt(patient_data, symptoms, medical_history) }}\n  \"#\n} ",
  "templates/risk-modeling.baml": "// Risk Modeling Template\ntemplate_string RiskModelingPrompt(risk_factors: string, patient_profile: string, outcome_type: string) #\"\nYou are an expert in medical risk assessment and predictive modeling for healthcare outcomes.\n\n**Risk Factors:**\n{{ risk_factors }}\n\n**Patient Profile:**\n{{ patient_profile }}\n\n**Outcome Type:**\n{{ outcome_type }}\n\n**Instructions:**\n1. Analyze the provided risk factors and patient profile\n2. Assess the relative importance of each risk factor\n3. Estimate probability of the specified outcome\n4. Identify modifiable vs. non-modifiable risk factors\n5. Suggest risk mitigation strategies\n6. Provide confidence intervals for risk estimates\n\n**Risk Assessment Framework:**\n- Quantitative risk scoring (0-100 scale)\n- Categorical risk levels (Low, Moderate, High, Critical)\n- Time-based risk projections (6 months, 1 year, 5 years)\n- Intervention impact analysis\n\n**Output Format:**\nPlease provide a structured risk assessment with:\n- Overall Risk Score and Category\n- Individual Risk Factor Analysis\n- Risk Timeline Projections\n- Modifiable Risk Factors\n- Recommended Interventions\n- Monitoring Recommendations\n- Confidence Assessment\n\"#\n\nfunction AssessRiskModel(\n  risk_factors: string,\n  patient_profile: string,\n  outcome_type: string\n) -> string {\n  client LocalOllama\n  prompt #\"\n    {{ _.role(\"system\") }}\n    {{ RiskModelingPrompt(risk_factors, patient_profile, outcome_type) }}\n  \"#\n} ",
}
export const getBamlFiles = () => {
    return fileMap;
}